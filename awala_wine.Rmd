---
title: "Wine Data Analysis"
author: "Dr. Awala Fortune"
date: "`r format(Sys.Date())`"
output:
  pdf_document: 
    citation_package: natbib
    df_print: kable
    highlight: tango
    keep_tex: yes
    number_sections: yes
    toc: yes
  html_document:
    df_print: kable
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
geometry:
- top=25mm
- bottom=25mm
- left=25mm
- right=25mm
- heightrounded
highlight-style: pygments 
linkcolor: blue
urlcolor: blue
mainfont: Arial
fontsize: 12pt
sansfont: Verdana
documentclass: report
  
---
 $$/pagebreak
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### I. Introduction###
 
The Capstone project is part of the requirement for Data Science Professional Certification  program hosted by HarvardX. The scope of this project covers application of basic understanding of R language, data visualization, probability and statistics, inference and modeling, application of productivity tools, data wrangling, linear regression and the application of machine learning in solving real life problems.

The University of California Irvine (UCI) machine learning repository is a center for machine learning  and intelligent systems, it currently maintains 504 data sets as a service to the machine learning community for analysis of algorithms.

##1.1. General description of Dataset:##

This project uses the wine quality data set of white and red wines of vinho verde wine samples, from the north of Portugal. The aim of the research is to model wine quality based on physicochemical properties such as  fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol using classification or regression model. The input is derived from the physicochemical test while the output is viewed as a sensory data such as quality.This outcomes are grouped in classes or categories for instance,  quality has 10 classes, numbered from 0 to 9, and wine type has two classes, red and white. 


### II. Methods and Analysis###
 
 
The research will apply several data science and machine learning classification techniques such as data cleaning, data exploration, visualization and modeling approach in Predicting wine quality and identifying the wine type, red or white in solving classification problems. The research therefore aimed at modeling wine quality based on physicochemical properties using machine learning classification techniques. 

Loading of Libraries and dataset

```{r}
#load libraries
library("ggplot2")
library("dplyr")
library("gridExtra")
library(Simpsons)
library(GGally)
library(memisc)
library(pander)
library(corrplot)
library(RCurl)
```
2.1. Data Preparation:
 
In this section, three files are downloaded from the UCI repository consisting of one csv file for each wine type and one file with the dataset description, the three files would be imported into Rstudio.

The column is added to each dataset to define the type of wine we want to predict with the factor variable type. This variable is used as the outcome that will be predicted.

Then, both datasets are combined in the wine dataset.
##2.2. Model Evaluation:##

The evaluation of machine learning algorithms has to do with comparing the predicted value with the actual outcome. The confusion matrix displays the predicted and observed values in a table and provides additional statistics summary.

##2.3. Statistical Terminology:##

#a. Confusion Matrix:#
The confusion matrix is a simple table with the cross tabulation of the predicted values with the actual observed values. Statistical metrics can be calculated from the confusion matrix.

#b. Accuracy:# 
This the proportion of correct predictions for both positive and negative outcomes. High accuracy with a large difference in the number of positives and negatives becomes less meaningful, due to the fact that the algorithm loses the ability to predict the less common class. In this case, other metrics complements the analysis.

#c. Sensitivity:#
This is the proportion of positive values when they are actually positive.

#d. Specificity:# 
The probability of a predicted negative value conditioned to a negative outcome. 


#f. Prevalence:# 
This term shows how often the positive value appears in the sample. Low prevalence may lead to statistically incorrect conclusions. For the purpose of this research would use interchangeably with ubiquity.


#e. Precision:# 
This is the probability of an actual positive occurrence conditioned to a predicted positive result.Precision can be written as the proportion of positive values that are actually positive.


#g. Recall:# 
This is the same as sensitivity and is the probability of a predicted positive value conditioned to an actual positive value.

#h. ROC and AUC:# 
Receiver Operating Characteristic (ROC) curves are a popular way to visualize the tradeoffs between sensitivitiy and specificity in a binary classifier. Computing the area under the curve is one way to summarize it in a single value; this metric is so common that if data scientists say â€œarea under the curveâ€ or â€œAUCâ€, you can generally assume they mean an ROC curve unless otherwise specified.
The area under the ROC curve (AUC) measures the probability that a model will rank a positive value higher than a negative one.AUC ranges from 0 to 1. A model with 100% wrong predictions has an AUC of 0, a model with 100% right predictions has AUC of 1 and a model with random prediction has an AUC of 0.5.

A guide for classifying the accuracy of a diagnostic test adopted from the traditional academic point system:

AUC	Classification
0.90 - 1	excellent (A)
0.80 - 0.90	good (B)
0.70 - 0.80	fair (C)
0.60 - 0.70	poor (D)
0.50 - 0.60	fail (F)



#i. The F1 Score:# 
The F1 score is a measure of accuracy for classification problems with binary outcome. It is computed as the harmonic mean between precision and recall.

It is observed that precision and recall vary from 0 to 1, therefore a plot of both values with the F1 score variation in color gradient is ploted. The black lines indicate the same F1 score values for different precision and recall combinations.
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
# Define F1 score function
F1_score <- function(prec, rec) {
  2 * (prec * rec) / (prec + rec)
}

# Create a dataframe with precision and recall values
w <- expand.grid(precision = seq(0, 1, .01), 
                 recall = seq(0, 1, .01))

tib <- tibble(precision = w$precision, 
              recall = w$recall, 
              F1 = F1_score(w$precision, w$recall))

# Create the plot
tib %>% ggplot(aes(precision, recall, z = F1, fill = F1)) +
  geom_raster() +
  labs(title = "F1 Score") +
  xlab("Precision") +
  ylab("Recall") +
  scale_fill_gradientn(colors=c("#F70D0D", "white", "#005DFF")) +
  # Draw countour lines
  stat_contour(breaks=c(0.1), color="black", na.rm = TRUE) +
  stat_contour(breaks=c(0.2), color="black", na.rm = TRUE) +
  stat_contour(breaks=c(0.3), color="black", na.rm = TRUE) +
  stat_contour(breaks=c(0.4), color="black", na.rm = TRUE) +
  stat_contour(breaks=c(0.5), color="black", na.rm = TRUE) +
  stat_contour(breaks=c(0.6), color="black", na.rm = TRUE) +
  stat_contour(breaks=c(0.7), color="black", na.rm = TRUE) +
  stat_contour(breaks=c(0.8), color="black", na.rm = TRUE) +
  stat_contour(breaks=c(0.9), color="black", na.rm = TRUE) +
  # Write the line levels
  geom_text(aes(x = 0.15, y = 0.1, label = "0.1")) +
  geom_text(aes(x = 0.25, y = 0.2, label = "0.2")) +
  geom_text(aes(x = 0.35, y = 0.3, label = "0.3")) +
  geom_text(aes(x = 0.45, y = 0.4, label = "0.4")) +
  geom_text(aes(x = 0.55, y = 0.5, label = "0.5")) +
  geom_text(aes(x = 0.65, y = 0.6, label = "0.6")) +
  geom_text(aes(x = 0.75, y = 0.7, label = "0.7")) +
  geom_text(aes(x = 0.85, y = 0.8, label = "0.8")) +
  geom_text(aes(x = 0.95, y = 0.9, label = "0.9"))

```
```{r echo=FALSE, message=FALSE, warning=FALSE}
options(digits = 3)
load_lib <- function(libs) {
  sapply(libs, function(lib) {
# Load the package. If it doesn't exists, install and load.
    if(!require(lib, character.only = TRUE)) {

      # Install the package
      install.packages(lib)
      
      # Load the package
      library(lib, character.only = TRUE)
      }
})}

# Load the libraries used in this section
libs <- c("tidyverse", "icesTAF", "readr", 
          "lubridate", "caret")

load_lib(libs)

# Download the datasets from UCI repository
if(!dir.exists("data")) mkdir("data")
if(!file.exists("data/winequality-red.csv")) 
  download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv", "data/winequality-red.csv")
if(!file.exists("data/winequality-white.csv")) 
  download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", "data/winequality-white.csv")
if(!file.exists("data/winequality.names")) 
  download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names", "data/winequality.names")

# Import the datasets.
# 'red' is the red wine dataset
# 'white' is the white wine dataset.
red   <- read_delim("data/winequality-red.csv", 
                    delim = ";", 
                    locale = locale(decimal_mark = ".", 
                                    grouping_mark = ","), 
                    col_names = TRUE)
white <- read_delim("data/winequality-white.csv", 
                    delim = ";", 
                    locale = locale(decimal_mark = ".", 
                                    grouping_mark = ","), 
                    col_names = TRUE)

# Set column names
cnames <- c("fixed_acidity", "volatile_acidity", "citric_acid",
            "residual_sugar", "chlorides", "free_sulfur_dioxide",
            "total_sulfur_dioxide", "density", "pH",
            "sulphates", "alcohol", "quality")

# Columns used for prediction are all columns
# except 'quality'.
xcol <- c("fixed_acidity", "volatile_acidity", "citric_acid",
          "residual_sugar", "chlorides", "free_sulfur_dioxide",
          "total_sulfur_dioxide", "density", "pH",
          "sulphates", "alcohol")

colnames(red)   <- cnames
colnames(white) <- cnames
# Add the column 'type' to define the type of wine
red   <- mutate(red,   type = "red")
white <- mutate(white, type = "white")

# Join 'red' and 'white' datasets
wine <- rbind(red, white)
wine <- mutate(wine, 
               quality = as.factor(quality),
               type = as.factor(type))
levels(wine$quality) <- paste0("Q", levels(wine$quality))
```

The dataset is split into two parts, one for training and one for testing. The model building and tuning is done in the training set, and then we use the test set to predict new values and evaluate the results.

We create a pair of training and testing sets for each prediction problem. train_set and test_set will be used for identifying wine type, and train_set_r and test_set_r will be used for red wine quality.

```{r echo=FALSE, message=FALSE, warning=FALSE }
# Test set will be 10% of the entire dataset
set.seed(2020, sample.kind = "Rounding")
test_index <- createDataPartition(y = wine$type, 
                                  times = 1, 
                                  p = 0.1, 
                                  list = FALSE)

# Train and test sets for wine type
train_set <- wine[-test_index,]
test_set  <- wine[test_index,]

# Train and test sets for red wine quality
train_set_r <- train_set[which(train_set$type == "red"),]
test_set_r  <- test_set[which(test_set$type == "red"),]

train_set_r$quality <- factor(train_set_r$quality)
test_set_r$quality  <- factor(test_set_r$quality)
```
###III.Results and Discussion###


##3.1. Data Exploration and Visualization:##

This section checks the dataset structure, look for possible problems in the data and provides a clear understanding of the data. The information acquired in this section may be used during the modeling phase.

Although UCI provides the dataset ready for analysis, itâ€™s good practice to check it for possible issues we may encounter in the future.

The number of empty values are counted to ascertain the NAs in the entire dataset in the training and testing sets. Empty values may cause calculation errors that can be avoided if we identify them in advance.

```{r}
#========================================
# Data Explorations
#========================================
# After importing the dataset, it's good practice to check the data.
# Here, we make some basic data checking.


# Check for empty values (NAs) in the dataset
sum(is.na(wine))
```
There are no empty values in the dataset.

```{r}
# Identification of near zero variance predictors
nearZeroVar(train_set[, xcol], saveMetrics = TRUE)
```
The dataset structure confirms the information provided by UCI and provides additional information. All variables are numeric except type which we added as a factor with two levels. There are 5847 observations and 13 variables.
```{r}
# Compactly Display the Structure of an Arbitrary R Object
str(train_set)

```

```{r}
head(train_set)
```
```{r}
tail(train_set)
```

The statistical summary provides a clue of each variable distribution.The observation of median close to the mean may be an indication of normal distribution. The variables free_sulfur_dioxide, total_sulfur_dioxide, density, pH, sulphates and alcohol seems to follow this rule that we will confirm later.

The relative numbers of red as 1439 and white as 4408 in the type variable give an idea of prevalence or ubiquity.
```{r}
# Statistics summary
summary(train_set)
```

```{r}
glimpse(train_set)
```

##3.2. Red and White Wine Ubiquity or Prevalence:##

The aim is to identify and classify red and white wines, it is imperative to understand the distribution pattern of both types by reviewing the relative amount.

```{r}
# Distribution of red and white wines
ggplot(data = train_set) + 
  geom_bar(aes(type, fill = type)) +
  labs(title = "Ubiquity of red and white wines",
       caption = "Source: train_set dataset.") +
  theme(legend.position = 'none')
```

The percentage of the prevalence of red wine in this dataset is 24.6% and that of the white wine is 75.4%.

In comparing the prevalence against the total production of red and white wines. The commission of vinho verde producers provides annual statistics of production6 for each wine type. The next step is to import the statistical data and calculate the prevalence of red wine production.
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Install and load the libraries used for visualization
# The 'load_lib' function was defined earlier.
load_lib(c("gridExtra", "ggridges", "ggplot2",
           "gtable", "grid", "egg"))


# The 'grid_arrange_shared_legend' function creates a grid of 
# plots with one legend for all plots.
# Reference: Baptiste AuguiÃ© - 2019
# https://cran.r-project.org/web/packages/egg/vignettes/Ecosystem.html
grid_arrange_shared_legend <-
  function(...,
           ncol = length(list(...)),
           nrow = 1,
           position = c("bottom", "right")) {
    
    plots <- list(...)
    position <- match.arg(position)
    g <-
      ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
    legend <- g[[which(sapply(g, function(x)
      x$name) == "guide-box")]]
    lheight <- sum(legend$height)
    lwidth <- sum(legend$width)
    gl <- lapply(plots, function(x)
      x + theme(legend.position = "none"))
    gl <- c(gl, ncol = ncol, nrow = nrow)
    
    combined <- switch(
      position,
      "bottom" = arrangeGrob(
        do.call(arrangeGrob, gl),
        legend,
        ncol = 1,
        heights = unit.c(unit(1, "npc") - lheight, lheight)
      ),
      "right" = arrangeGrob(
        do.call(arrangeGrob, gl),
        legend,
        ncol = 2,
        widths = unit.c(unit(1, "npc") - lwidth, lwidth)
      )
    )
    
    grid.newpage()
    grid.draw(combined)
    
    # return gtable invisibly
    invisible(combined)
    
  }
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
load_lib(c("readxl", "huxtable", "viridis", "ggthemes"))

# The 'huxtable' package creates beautiful tables.
# 'viridis' and 'ggthemes' have color paletes for color blind people

# Download stats file from vinho verde official portal
if(!file.exists("data/vv-stats.xls")) 
  download.file("https://portal.vinhoverde.pt/pt/file/c/1614",
                "data/vv-stats.xls",
                cacheOK = FALSE,
                method = "auto",
                mode = "wb")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Import stats file
vv_stats <- read_excel(path = "data/vv-stats.xls",
                       sheet = "vinho",
                       range = "A6:D16")

# Calculate the prevalence of red wine
vv_stats <- vv_stats[2:nrow(vv_stats),c(1,3:4)] %>% 
  mutate(Prevalence = 100 * TINTO / BRANCO)

# Create a table with the values.
# Change column names
colnames(vv_stats) <- c("Year", "White", "Red", "Red Prevalence (%)")
vv_stats <- as_hux(vv_stats)

vv_stats <- huxtable::add_colnames(vv_stats)

vv_stats <- vv_stats %>%
  # Format header row
  set_bold(row = 1, col = 1:ncol(vv_stats), value = TRUE)        %>%
  set_top_border(row = 1, col = 1:ncol(vv_stats), value = 1)     %>%
  set_bottom_border(row = c(1,10), col = 1:ncol(vv_stats), value = 1)  %>%
  # Format cells
  set_align(row = 1:4, col = 2, value = 'right')                 %>%
  set_number_format(row = 1:nrow(vv_stats), col = c(2,3), 
                    value = list(function(x)
                      prettyNum(x, big.mark = ",",
                                scientific = FALSE)))            %>% 
  set_number_format(row = 1:nrow(vv_stats), col = 4, value = 2)  %>% 
  # Format table
  set_width(value = 0.6) %>%
  set_caption("Vinho Verde Annual Production 1999-2008")         %>%
  set_position(value = "center")

# Show the table
vv_stats

```
There is no information available about production by brand or grape variety but from the studies it is observed that the prevalence of the production of red wine is higher than that observed in the dataset.


##3.3. Quality distribution:##

For a better prediction of quality distribution of red wines there is the need to understand the distributional pattern. It is observed from the result that the quality distribution is highly disproportional and there are many more wines with qualities 5, 6 and 7 than those with qualities 3,4 and 8 as seen in the barplot below.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Create a plot with the downloaded data.
# The plot is easier to see the values than in the table
# Distribution of quality values
ggplot(data = train_set_r, 
       aes(x = quality, fill ='red')) +
  geom_bar() +
  theme(legend.position="none") +
  labs(title = "Distribution of quality of red wine",
       caption = "Source: train_set_r dataset")
```

##3.4. Important Variables:##

In prediction of the wine types there are 11 variables or features that may be used as predictors.The R function filterVarImp is used to estimate the importance of each variable. For the two class outcomes, the area under the ROC curve is used as a measure of variable importance.

From the result obtained it is observed that the total sulfur dioxide, chlorides and volatile acidity are the most important variable for predicting wine type, while alcohol, citric acid and residual sugar are the least important.

```{r}
#---------------------------------------
# Important Variables 
#---------------------------------------
# The important variable gives an estimate of the predictive power
# of each feature. 
# Check the help file for 'filterVarImp' for more information.

# Variable importance for wine type
hux(Feature = rownames(filterVarImp(x = train_set[,xcol], 
                                    y = train_set$type)),
    Red   = filterVarImp(x = train_set[,xcol], 
                         y = train_set$type)$red,
    White = filterVarImp(x = train_set[,xcol],
                         y = train_set$type)$white,
    add_colnames = TRUE) %>%
  arrange(desc(Red)) %>% 
  set_bold(row = 1, everywhere, value = TRUE)          %>%
  set_top_border(row = 1, everywhere, value = 1)    %>%
  set_bottom_border(row = c(1,12), everywhere, value = 1)    %>%
  set_align(row = everywhere, col = 2:3, value = 'right') %>%
  set_caption('Important Variables of Wine Type') %>%
  set_position(value = "center")
```
The highly important features are used to construct a geompoint plot against itself to ascertain the association.
```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data = train_set, aes(x = volatile_acidity, y = total_sulfur_dioxide )) +
  geom_point(alpha = 0.3) +
  scale_x_log10(breaks=seq(.1,1,.1)) +
  xlab("Volatile Acidity in Log Scale") +
  geom_smooth(method="lm")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data = train_set, aes(x = volatile_acidity, y = chlorides )) +
  geom_point(alpha = 0.3) +
  scale_x_log10(breaks=seq(.1,1,.1)) +
  xlab("Volatile Acidity in Log Scale") +
  geom_smooth(method="lm")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data = train_set, aes(x = chlorides, y = total_sulfur_dioxide )) +
  geom_point(alpha = 0.3) +
  scale_x_log10(breaks=seq(.1,1,.1)) +
  xlab("chlorides in Log Scale") +
  geom_smooth(method="lm")
```

In predicting the wine quality in a scale of 1 to 9 in order to obtain a multi-class outcome for a specific class, the problem is decomposed into a pair-wise problem and the area under the curve is calculated for each class, hence the maximum area under the curve across the relevant pair-wise AUCâ€™s is used as an important variable measure. From the result the volatile acidity, sulphates and alcohol have very high AUC values for red wine quality 

```{r}
#------------------
#Important Variables  for red wine quality
#------------------
x <- train_set_r[,xcol]
y <- train_set_r$quality

hux(Feature = rownames(filterVarImp(x = x, y = y)),
    filterVarImp(x = x, y = y),
    add_colnames = TRUE) %>%
  # Format header row
  set_bold(row = 1, everywhere, value = TRUE)          %>%
  set_top_border(row = 1, everywhere, value = 1)       %>%
  set_bottom_border(row = c(1,12), everywhere, value = 1)    %>%
  # Format numbers
  set_number_format(row = 2:12, col = 2:7, value = 3)  %>%
  
  
  map_text_color(row = everywhere, col = 2:7, 
                 by_ranges(seq(0.6, 0.9, 0.1), colorblind_pal()(5))) %>%
  # Format alignment
  set_align(row = everywhere, col = 1,   value = 'left')  %>%
  set_align(row = everywhere, col = 2:7, value = 'right') %>%
  # Title
  set_caption('Important variables of red Wine quality') %>%
  set_position(value = "center")

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Here we create a plot of variable information of wine quality.
# The same info as in the table above.
# Variable importance for red wine quality
x <- train_set_r[,xcol]
y <- train_set_r$quality
y <- factor(y)

data.frame(Feature = rownames(filterVarImp(x = x, y = y)),
           filterVarImp(x = x, y = y)) %>%
  pivot_longer(col = 2:7, names_to = "Quality",
               values_to = "Value", values_drop_na = TRUE) %>%
  ggplot(aes(x = Feature, y = Value)) +
  geom_col(fill = "blue") +
  coord_flip() +
  ggtitle("Important Variables of red wine quality") +
  theme(legend.position = "none") +
  ylab("Relative Importance") +
  geom_hline(yintercept = seq(0.5, 0.9, 0.1), color = "red") +
  facet_wrap("Quality")
```
From the relative importance of the features there is an overlapping for different quality outcome, it is therefore important to utilize machine learning models that have the capacity to use two or more features for prediction, thereby solving the problem of low predictive power as a result of highly correlated features.

A look at each feature distribution for each wine type and quality of red wines would be studied and the correlation among the features presented.

```{r eval=FALSE, message=FALSE, warning=FALSE}
#========================================
# Data visualization
#========================================
# In this section we create several stats plots
# to check the distribution of variables.

# Install and load the libraries used for visualization
# The 'load_lib' function was defined earlier.
load_lib(c("gridExtra", "ggridges", "ggplot2",
           "gtable", "grid", "egg"))


# The 'grid_arrange_shared_legend' function creates a grid of 
# plots with one legend for all plots.
# There's no commentaries because I use the code from the source below.
# Reference: Baptiste AuguiÃ© - 2019
# https://cran.r-project.org/web/packages/egg/vignettes/Ecosystem.html
grid_arrange_shared_legend <-
  function(...,
           ncol = length(list(...)),
           nrow = 1,
           position = c("bottom", "right")) {
    
    plots <- list(...)
    position <- match.arg(position)
    g <-
      ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
    legend <- g[[which(sapply(g, function(x)
      x$name) == "guide-box")]]
    lheight <- sum(legend$height)
    lwidth <- sum(legend$width)
    gl <- lapply(plots, function(x)
      x + theme(legend.position = "none"))
    gl <- c(gl, ncol = ncol, nrow = nrow)
    
    combined <- switch(
      position,
      "bottom" = arrangeGrob(
        do.call(arrangeGrob, gl),
        legend,
        ncol = 1,
        heights = unit.c(unit(1, "npc") - lheight, lheight)
      ),
      "right" = arrangeGrob(
        do.call(arrangeGrob, gl),
        legend,
        ncol = 2,
        widths = unit.c(unit(1, "npc") - lwidth, lwidth)
      )
    )
    
    grid.newpage()
    grid.draw(combined)
    
    # return gtable invisibly
    invisible(combined)
    
  }
```
##3.5 Density Plot##
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Density grid
dens_grid <- lapply(xcol, FUN=function(var) {
  # Build the plots
  ggplot(train_set) + 
    geom_density(aes_string(x = var, fill = "type"), alpha = 0.5) +
    ggtitle(var)
})
do.call(grid_arrange_shared_legend, args=c(dens_grid, nrow = 4, ncol = 3))
```

The general distribution of the variables are presented in the geomplot above. The different variables are arranged in threes to obtain a larger plot as presented below. The difference in the peaks for red and white wine suggest that the predictors would be useful in the distinguishing of both wine types. The shifting to the left side of the x-axis indicates possible outliers in the right side.

The comparative studies of the two types of wine, the red and white using three variables such as alcohol, PH and citric_acid displayed shows that the alcohol level is higher in the red wine having a peak density of 0.5 while the white wine peaked at 0.35. The citric_acid level is higher in the white wine at density of 5 while the red wine has a lower citric_acid. The PH level is equal in the two wines.  

The white wine contains higher density of volatile_acidity, higher chlorides and lower total_sulfur_dioxide while the red wine possess higher total_sulfur_dioxide, lower volatile_acidity and lower chlorides. 

In distiquishing the two types of wine based on the variables, the sulphates content and the fixed_acidity in the white wine is relatively higher comparing to that of red wine, while the red wine contains hiher free_sulphur_dioxide comparing to the red wine.  

The residual_sugar and density peaked higher in red wine compared to the white wine while the white wine is relatively higher in sulphates content.
```{r echo=FALSE, message=FALSE, warning=FALSE}
# The distribution of predictors overlap for all quality levels.
# Maybe if we group the quality levels there's less overlap.
# Here we create 2 datasets to predict wine quality on the new levels.
train_set_r <- train_set_r %>% 
  mutate(quality2 = factor(case_when(
    quality %in% c("Q3", "Q4") ~ "low",
    quality %in% c("Q5", "Q6") ~ "medium",
    quality %in% c("Q7", "Q8") ~ "high"),
    levels = c("low", "medium", "high")))

test_set_r <- test_set_r %>% 
  mutate(quality2 = factor(case_when(
    quality %in% c("Q3", "Q4") ~ "low",
    quality %in% c("Q5", "Q6") ~ "medium",
    quality %in% c("Q7", "Q8") ~ "high"),
    levels = c("low", "medium", "high")))

# Plot the distribution of new quality levels
train_set_r %>% ggplot(aes(quality2, fill = quality2)) + geom_bar()
```
##3.6  Box Plot##

Box plots show a summary of the main values in the distribution of red wine that is the lower and upper limits, the first and third quantiles, the mean and outliers. The distribution overlaps among all quality levels for all predictors. The mean also is flat for all quality levels and predictors, except for citric acid and sulphates that increases with quality levels and volatile acidity that decreases.This further butresses the fact that the distribution is higher in Q5 and Q6 which categorized together as medium. 

All features have outliers.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Now we try to find the relationship between 'quality' and 
# the features.
#
# Another boxplot to check if the grouping improved the overlaps.
# Arrange the dataset
train_set_r[,c(cnames, "quality2")] %>% 
  pivot_longer(cols = -c(12:13), 
               names_to = "Feature", 
               values_to = "Value") %>%
  # Create the box plot
  ggplot(aes(x = quality2, y= Value, fill = quality2)) +
  geom_boxplot() +
  # Format labels
  #  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("") +
  ggtitle("Quality of Red Wine by feature") +
  # Create grid by feature
  facet_wrap(. ~ Feature, scales = "free", ncol = 3, shrink = FALSE)
```

From the result earlier obtained there is no clear distinction of quality levels among the predictors. Maybe grouping the quality values may help. We would denote low quality for levels 3 and 4, medium for levels 5 and 6, and high for levels 7 and 8.

##3.7.  Quantile-Quantile plot:##
A Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that is roughly straight.

The QQ plots compare the feature distribution with the normal distribution. All variables are close to the normal distribution within two standard deviations of the mean. Some machine learning algorithms, such as Linear Discriminant Analysis (LDA) and Quadractic Discriminant Analysis(QDA), assume the predictors are normally distributed.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#------------------
# Density ridge plots
#------------------
# Predict red wine quality
# It's easier to see the distribution with density ridge plots.
# It plots each quality level in a different row.
lapply(xcol, FUN=function(var) {
  
  train_set_r %>% 
    ggplot(data = ., aes_string(x = var, 
                                y = "quality", 
                                fill = "quality", 
                                alpha = 0.5)) + 
    geom_density_ridges() +
    theme_ridges() +
    theme(axis.text.x = element_text(hjust = 1)) +
    scale_fill_brewer(palette = 4) +
    ggtitle(paste0("Red wine quality by ", var))
})

#------------------
# QQ plots
#------------------
# QQ Plots help identify if the feature is normally distributed.
#
# Create a grid for each wine type
# 'rw' = red / white
qq_plot_grid <- lapply(xcol, FUN=function(var) {
  train_set_r %>% 
    dplyr::select(var) %>%
    ggplot(data = ., aes(sample = scale(.))) + 
    stat_qq() +
    stat_qq_line(colour = "red") +
    theme(axis.text.x = element_text(hjust = 1)) +
    ggtitle(var)
})
do.call(grid.arrange, args=c(qq_plot_grid, list(ncol=3)))
```

##3.8.  Density Ridge##  

The density ridge plots shows the different physicochemical variables and its quality. The ridgeline plots are partially overlapping line plots that create the impression of a mountain range and are used for visualizing changes in distributions over time or space.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Density ridge plots
lapply(xcol, FUN=function(var) {
  
  train_set_r %>% 
    ggplot(data = ., aes_string(x = var, 
                                y = "quality2", 
                                fill = "quality2", 
                                alpha = 0.5)) + 
    geom_density_ridges() +
    # Format chart
    theme_ridges() +
    scale_fill_brewer(palette = 4) +
    # Format labels
    theme(axis.text.x = element_text(hjust = 1)) +
    ggtitle(paste0("Red wine quality by ", var)) +
    ylab("Quality")
})
```

##3.9.  Correlation:## 

Correlogram is a graph of correlation matrix. It is very useful to highlight the most correlated variables in a data table. In this plot, correlation coefficients is colored according to the value. Correlation matrix can be also reordered according to the degree of association between variables.
The exploration done so far shows that volatile acid, chlorides and total sulfur dioxide are good candidates as wine type predictors. The prediction may be lower than expected if any two of these variables are highly correlated.

The correlogram shows the correlation of all predictors in pairs. The diagonal contains the variable names. The correlation of two variables are in the intersection of the variables rows and columns. The color intensity of the boxes and numbers indicate the correlation degree: strong colors indicate high correlation, whereas light colors indicate low correlation.

The assumption in this analysis is that low correlation is between -0.5 and 0.5, and high correlation otherwise. Most colors in the correlogram are light and the correlations are low.
```{r echo=FALSE, message=FALSE, warning=FALSE}
#------------------
# Correleation
#------------------
# We want features with low correlation with each other.
# Load the "corrgram" package to draw a correlogram
load_lib("corrgram")

# Draw a correlogram
corrgram(train_set[,xcol], order=TRUE, 
         lower.panel = panel.shade, 
         upper.panel = panel.cor, 
         text.panel  = panel.txt,
         main = "Correlogram: Wine Physicochemical Properties",
         col.regions=colorRampPalette(c("red", "burlywood1", "blue",
                                         "darkgreen")))

# The correlogram has many information, so we filter only
# the features with high correlation and show in a table.
# 'High' correlation here is above 0.5 and lower than -0.5.
#
# Load the 'huxtable' package to format tables
load_lib("huxtable")
options(huxtable.knit_print_df = FALSE)

# Column names, same as 'xcol' but more beautiful
variable_names <- c("Fixed acidity", "Volatile acidity", "Citric acid",
               "Residual sugar", "Chlorides", "Free sulfur dioxide",
               "Total sulfur dioxide", "Density", "pH", "Sulphates",
               "Alcohol")
```

The table below summarizes the pairs of variables with high correlation.

The correlations of volatile acid, chlorides and total sulfur dioxide are low as against the highly correlated variables listed in the table.

```{r}
# Calculate the correlation of all predictors
my_cor <- as.data.frame(cor(train_set[,xcol]))

# Row (r) and column (c) numbers of the correlation matrix
# filtered by high correlated features (cor <= -0.5 or cor >= 0.5)
r <- which(my_cor <=-0.5 | my_cor >= 0.5 & my_cor != 1, arr.ind=TRUE)[,"row"]
c <- which(my_cor <=-0.5 | my_cor >= 0.5 & my_cor != 1, arr.ind=TRUE)[,"col"]

# Create a table with high correlations features only
my_cor_hux <- hux(`Feature 1` = variable_names[r], 
                  `Feature 2` = variable_names[c], 
                  Correlation = sapply(1:length(r), function(x)
                    my_cor[r[x],c[x]]),
                  add_colnames = TRUE) %>%
  # Format the table
  set_bold(row = 1, everywhere, value = TRUE)          %>%
  set_top_border(row = 1, everywhere, value = 1)    %>%
  set_bottom_border(row = c(1,7), everywhere, value = 1)    %>%
  set_align(row = 1, col = 2:3, value = 'center') %>%
  set_number_format(row = 2:7, col = 3, value = 3)              %>% 
  set_caption('High Correlated Features') %>%
  set_position(value = "center")

# Show the table
my_cor_hux
```

```{r}
# Previously, we identified 3 features that may be used in prediction
# of wine type. We create a table to check if the correlation 
# between each pair is low.


# Calculate the correlations for volatile acid, chlorides and total sulfur dioxide

# Variable names
pred1 <- c("volatile_acidity", "chlorides", "total_sulfur_dioxide")

# Nice variable names
xpred2 <- c("Volatile acidity", "Chlorides", "Total sulfur dioxide")

# Calculate the correlation of all predictors
my_cor2 <- as.data.frame(cor(train_set[,pred1]))

# Create a table with high correlations features only
cor_hux2 <- hux(cor(train_set[,pred1]),
                   add_colnames  = FALSE, 
                   add_rownames = FALSE) 

# Set row and column names
rownames(cor_hux2) <- xpred2
colnames(cor_hux2) <- xpred2
cor_hux2 <- add_rownames(cor_hux2, colname = "Feature")
cor_hux2 <- add_colnames(cor_hux2, value = TRUE)

# Format the table
cor_hux2 <- cor_hux2 %>%
  set_bold(row = 1, everywhere, value = TRUE)          %>%
  set_top_border(row = 1, everywhere, value = 1)    %>%
  set_bottom_border(row = c(1,4), everywhere, value = 1)    %>%
  set_align(row = 1, col = 2:ncol(my_cor_hux), value = 'center') %>%
  set_number_format(everywhere, everywhere, value = 3)              %>% 
  set_caption('Correlation Matrix - Selected Features') %>%
  set_position(value = "center") %>%
  set_width(value = 0.6)

# Show the table
cor_hux2
```

##3.10.  Modeling##

It is observed in data exploration  that total sulfur dioxide, chlorides and volatile acidity are good candidates for wine type prediction. The AUC is very high, the distributions have low overlapping areas and the correlations are low.

Although the quality prediction of red wines may be challenging due to the fact that some features with high AUC have low correlations and all features present large distribution overlapping areas. Further more, the prevalence of wines with qualities 3, 4 and 8 is very low.

Subsequently, different modeling approaches would be utilized to predict red and white wines and wine quality.

#3.10.1. Simple Model:#

The simplest model is based on the assumption that all wines are red or white. Since there are more white than red wines, the model predicts all wines are white. The accuracy at 75.4 is higher than 50%, the specificity is 1 and sensitivity 0. The model is good at predicting white wines, but would not be very useful at predicting red wines. A better model should have higher sensitivity, although it may lower specificity and accuracy.

#3.10.2. Random Model:#

The second model uses the sample probabilities of each wine type as a predictor. The ubiquity of the red and white wines is 24.6% and 75.4% respectively, so the red or white is randomly assigned using the proportions. The problem experienced with this approach is that the actual distribution in the population is not known, and the actual distribution may fluctuate over time. To have a better performance any machine learning model should perform better than both models.

#3.10.3. The Single Predictor:#

Since the distributions of total sulfur dioxide, chlorides and volatile acidity stratified by wine type have small overlapping areas, we can find a cutoff value that maximizes the conditional probability.

Pr(Y=k|X=x)
Where Y is the outcome and p is the number of predictors.

because three predictors have been selected during data exploration, our model therefore becomes:

Pr(Y=k|X1=x1,X2=x2,X3=x3)=Î²0+Î²1x1+Î²2x2+Î²3x3

where x1 is total sulfur dioxide, x2 is chlorides and x3 is volatile acidity.

Comparatively the difference with the previous model is that linear regression builds a function that best fits the data.

#3.10.4. The K-Nearest Neighbors (KNN):#

The conditional probability are estimated using the k-nearest neighbors algorithm:
p(x1,..,xp)=Pr(Y=k|X1=x1,..,Xp=xp)
The algorithm calculates the euclidean distance of all predictors, for any point (x1,..,xp) in the multidimensional space that is being predicted, the algorithm determines the distance to k points. Hence, the k nearest points is refereed to as neighborhood.

For k=1 the algorithm finds the distance to a single neighbor. For k equals to the number of samples, the algorithm uses all points. Hence, k is a tuning parameter that can be calculated running the algorithm for several values of k and picking the result with highest accuracy or AUC.

#3.10.4. Classification and Regression Trees:#

The Classification and regression tree(CART) model is one of the oldest and most fundamental algorithms. It is used to predict outcomes based on certain predictor variables that is explaining how a target variableâ€™s values can be predicted based on other values.
They are seen as a decision tree where each fork is a split in a predictor variable and each node at the end has a prediction for the target variable. The tree is basically a flow chart of yes or no questions.

However, they do not have the best accuracy, they are hard to train and they are unstable to changes in the data, but are also the basis for other powerful machine learning algorithms like random forest and boosted decision trees.

#3.10.4.  Random Forest:#

Random forests or random decision forests are an ensemble learning model for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.

Random forests improve prediction performance over classification trees by averaging multiple decision trees. The algorithm creates several random subsets of the original data, in this case the training set, and calculates the classification trees, then the final result is the average of all trees.


#3.10.5.  Linear and Quadratic Discriminant Analysis:#

Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data.

In LDA and QDA the assumption is that all predictors are normally distributed and have the same standard deviations.The decision boundary of LDA is linear, while QDA is an extension of LDA with quadratic boundaries. They are all used to solving classification problems where the output variable is categorical. LDA supports both binary and multi-class classification.

The LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data P(X|y=k) for each class k. Predictions can then be obtained by using Bayesâ€™ rule:

Pr(y=k|X)=Pr(X|y=k)Pr(y=k)Pr(X)=Pr(X|y=k)Pr(y=k)âˆ‘lPr(X|y=l)â‹…Pr(y=l)
and we select the class k which maximizes this conditional probability.

LDA and QDA work better with few predictors.

#3.10.6.  Cross Validation:#

Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting.In ML, you can use the k-fold cross-validation method to perform cross-validation. In k-fold cross-validation, you split the input data into k subsets of data (also known as folds).

The original dataset is partitioned in the training set used to train the model, and the test set used to predict the values with the trained model. Cross validation partitions the training set in the same way and performs the training and prediction several times. Then, the result with the best RMSE, accuracy, AUC or the chosen metric is selected. This process can be used in conjunction to tuning parameters, for example picking the best k number of neighbors in knn.

#3.10.7  Ensemble:#

Ensemble is a technique that combines several base models in order to produce one optimal predictive model. Combining the results of several predictions may improve prediction accuracy. This method is called ensemble and consists in selecting the most common class for a given set of observations.

For example, suppose the predictions of wine type for three models as illustrated in the table below. The ensemble is just the majority of votes of the combined models.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#========================================
# Modeling and results
#========================================
# Here we make predictions with information gained from
# data exploration and visualization.
#

# Formula used in predictions
pml <- as.formula(paste("type", "~", 
                        paste(xcol, collapse=' + ')))
#------------------
# Single predictor
#------------------
# Predict wine type with total_sulfur_dioxide + chlorides + volatile_acidity
# The first prediction is very simple. We predict 'red' if
# the feature value is above a certain cutoff value, and 'white' 
# otherwise. 
# We do this for the 3 best features discorevered in data exploration.
# Then we combine the results in a single ensemble.
#
# Create a list with variable names and cutoff decision rule.
# If the predicted value is lower than the cutoff value, the first color
# is chosen, otherwise the second. To understand this, look at the
# density plots in data visualization.
t_var <- list( c("white", "red"), c("white", "red"), c("red", "white"))
names(t_var) <- c("volatile_acidity", "chlorides", "total_sulfur_dioxide")


# Create an empty results table. The first row
# contains NAs and will be removed after the predictions.
t_results <<- data.frame(Feature = NA,
                            Accuracy = NA,
                            Sensitivity = NA,
                            Specificity = NA,
                            stringsAsFactors = FALSE)

# Prediction function
preds <- sapply(1:length(t_var), function(x){
   var <- names(t_var[x])
  
  # Cutoff value is the distribution range divided by 500
  cutoff <- seq(min(train_set[,var]), 
                max(train_set[,var]), 
                length.out = 500)
  
  # Calculate accuracy
  acc <- map_dbl(cutoff, function(y){
    type <- ifelse(train_set[,var] < y, t_var[[x]][1], 
                   t_var[[x]][2]) %>% 
      factor(levels = levels(train_set$type))
    
    # Accuracy
    mean(type == train_set$type)
  })
  
  # Build the accuracy vs cutoff curve
  acc_plot <- data.frame(cutoff = cutoff, Accuracy = acc) %>%
    ggplot(aes(x = cutoff, y = Accuracy)) + 
    geom_point() +
    ggtitle(paste0("Accuracy curve for ", var))
  
  # Print the plot
  print(acc_plot)
   # Predict new values in the test set
  # The model uses the cutoff value with the best accuracy.
  max_cutoff <- cutoff[which.max(acc)]
  p_hat <- ifelse(test_set[,var] < max_cutoff,
                  t_var[[x]][1], t_var[[x]][2]) %>% 
    factor(levels = levels(test_set$type))
  
  # Calculate accuracy, specificity and sensitivity
  acc <- max(acc)
  sens <- sensitivity(p_hat, test_set$type)
  spec <- specificity(p_hat, test_set$type)
  
  # Update results table
  t_results <<- rbind(t_results,
                         data.frame(Feature = names(t_var[x]),
                                    Accuracy = acc,
                                    Sensitivity = sens,
                                    Specificity = spec,
                                    stringsAsFactors = FALSE))
  
  # The prediction will be used in the ensemble
  return(p_hat)
})  
```

```{r}
# Remove first row with NA
t_results <- t_results[2:nrow(t_results),]

# Combine the results using majority of votes
p_hat_ens <-as_factor(data.frame(preds) %>%
                        mutate(x = as.numeric(preds[,1] == "red") + 
                                 as.numeric(preds[,2] == "red") + 
                                 as.numeric(preds[,3]  == "red"),
                               p_hat = ifelse(x >=2, "red", "white")) %>%
                        pull(p_hat))

# Update results table
t_results <<- rbind(t_results,
                       data.frame(Feature = "Ensemble",
                                  Accuracy = mean(p_hat_ens == test_set$type),
                                  Sensitivity = sensitivity(p_hat_ens, test_set$type),
                                  Specificity = specificity(p_hat_ens, test_set$type),
                                  stringsAsFactors = FALSE))
as_hux(t_results,
       add_colnames = TRUE) %>%
  # Format header row
  set_bold(row = 1, everywhere, value = TRUE)          %>%
  set_top_border(row = 1, everywhere, value = 1)       %>%
  set_bottom_border(row = c(1,5), everywhere, value = 1)    %>%
  # Format numbers
  set_number_format(row = everywhere, col = 2:4, value = 3)  %>%
  # Format alignment
  set_align(row = everywhere, col = 1,   value = 'left')  %>%
  set_align(row = everywhere, col = 1:4, value = 'right') %>%
  set_caption("Best Performance for Combined Predictions")  %>%
  set_position(value = "center")


```

##3.11.  Linear Regression and KNN:##
Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Linear regression approximates the multidimensional space of predictors X and outcomes Y into a function. The outcome, which is wine type, is categorical, so we need to convert to number before building the function. Now, 1 would be assigned to red wine and 0 to white wine.

The predicted values are also numeric, so we need to convert back to categories.

Again, only the three main features would be used to predict wine type: total sulfur dioxide, chlorides and volatile acidity.

```{r}
#------------------
# Linear Regression
#------------------
# Predict wine type with total_sulfur_dioxide + chlorides + volatile_acidity

# Train the linear regression model
ft_lm <- train_set %>% 
  # Convert the outcome to numeric
  mutate(type = ifelse(type == "red", 1, 0)) %>%
  # Fit the model
  lm(type ~ total_sulfur_dioxide + chlorides + volatile_acidity, data = .)

# Predict
f_hat_lm <- predict(ft_lm, newdata = test_set)

# Convert the predicted value to factor
p_hat_lm <- factor(ifelse(f_hat_lm > 0.5, "red", "white"))

# Evaluate the results
caret::confusionMatrix(p_hat_lm, test_set$type)


#------------------
# Knn
#------------------
# Predict wine type with all features
# Train
ft_knn <- knn3(formula = pml, data = train_set, k = 5)

# Predict
p_knn <- predict(object = ft_knn, 
                 newdata = test_set, 
                 type ="class")

# Compare the results: confusion matrix
caret::confusionMatrix(data = p_knn, 
                       reference = test_set$type, 
                       positive = "red")

# F1 score
F_meas(data = p_knn, reference = test_set$type)
```

##3.12.  Regression tree - rpart:##

This is a regression part of CART. The basic regression trees partition a data set into smaller subgroups and then fit a simple constant for each observation in the subgroup. The partitioning is achieved by successive binary partitions also known as recursive partitioning, based on the different predictors. The constant to predict is based on the average response values for all observations that fall in that subgroup.
The regression tree for this studies uses all features to predict wine type.

```{r}
#------------------
# Regression tree
#------------------
# Predict wine type with all features

# The "rpart" package trains regression trees and 
# "rpart.plot" plots the tree
load_lib(c("rpart", "rpart.plot"))

# Train the model
ft_rpart <- rpart::rpart(formula = pml, 
                          method = "class", 
                          data = train_set)
# Predict
p_rpart <- predict(object = ft_rpart, 
                   newdata = test_set, 
                   type = "class")

# Compare the results: confusion matrix
caret::confusionMatrix(data = p_rpart, 
                       reference = test_set$type, 
                       positive = "red")

# Plot the result
rpart.plot(ft_rpart)

# F1 score
F_meas(data = p_rpart, reference = test_set$type)

# Variable importance
caret::varImp(ft_rpart)
```
Alcohol and citric acid have no importance at all while chlorides, total sulfur dioxide and volatile acidity are the three most important variables.

Each node shows:

a. the predicted class (red or white),
b. the predicted probability of that class,
c. the percentage of observations in the node.
The intensity of a nodeâ€™s color is proportional to the value predicted at the node hence, the resulting tree represents the rule to predict the different wine types.

##3.13. Random Forest:##
From the result of the random forest high sensitivity and specificity achieved.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#------------------
# Random Forest
#------------------
# Predict wine type with all features

# The "randomForest" package trains classification and regression
# with Random Forest
load_lib("randomForest")

# Train the model
ft_rforest <- randomForest(formula = pml, data = train_set)

# Predict
p_rf <- predict(object = ft_rforest, newdata = test_set)

# Compare the results: confusion matrix
caret::confusionMatrix(data = p_rf, 
                       reference = test_set$type, 
                       positive = "red")
```
```{r}
# F1 score
F_meas(data = p_rf, reference = test_set$type)
```
The error for red wines is higher than white wines this is as a result of lower ubiquity or prevalence.The error decreases as the number of trees grows, stabilizing around 50 trees. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Plot the error curve
data.frame(ft_rforest$err.rate) %>% mutate(x = 1:500 ) %>% 
  ggplot(aes(x = x)) + 
  #  geom_line(aes(y = OOB)) +
  geom_line(aes(y = red),   col = "red") +
  geom_line(aes(y = white), col = "purple") +
  ggtitle("Random Forest Error Curve") +
  ylab("Error") +
  xlab("Number of trees") +
  geom_text(aes(x = 70,  y = 0.02), label = "Red wine", col = "red") + 
  #  geom_text(aes(x = 100, y = 0.01), label = "Error") +
  geom_text(aes(x = 100, y = 0), label = "White wine", col = "purple")
```

The result shows that alcohol, citric acid and pH are the three most important variables for random forest prediction  while  chlorides and volatile acidity have low predictive power . 

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Variable importance plot
varImpPlot(ft_rforest, main = "Random Forest Important Variables")
```

##3.14 Linear Discriminant Analysis - LDA:##
LDA uses all features to predict wine type.The group of the red wine fell within the negative scale (-5 to 0) while the white wine fell within positive scale (0 to 5) indicating higher ubiquity or prevalence in the white wine. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
#------------------
# LDA
#------------------
# Predict wine type with all features
load_lib("MASS")

# Train the model
ft_lda <- lda(formula = pml, data = train_set)
# Predict
p_lda <- predict(object = ft_lda, newdata = test_set)

# Compare the results: confusion matrix
caret::confusionMatrix(data = p_lda[[1]], 
                       reference = test_set$type, 
                       positive = "red")

# F1 score
F_meas(data = p_lda[[1]], reference = test_set$type)

# Plot the result
plot(ft_lda)

```

##3.15  Quadratic Discriminant Analysis - QDA:##

QDA uses all features to predict wine type, the result further suggest that when red is predicted to be 158 white wine is predicted to be 486 and the accuracy is given as 0.991 and an F score of 98 percent.
```{r}
#------------------
# QDA
#------------------
# Predict wine type with all features
load_lib(c("MASS", "scales"))

# Train the model
ft_qda <- qda(formula = pml, data = train_set)

# Predict
p_qda <- predict(object = ft_qda, newdata = test_set)

# Compare the results: confusion matrix
caret::confusionMatrix(data = p_qda[[1]], 
                       reference = test_set$type, 
                       positive = "red")
# F1 score
F_meas(data = p_qda[[1]], reference = test_set$type)
```

##3.16 Prediction Results Evaluation:##

Random forest and Linear discriminant analysis (LDA) have the best performance,  although the single predictor model and linear regression models used three features as predictors, they showed better performance than knn that used all the features.


```{r echo=FALSE, message=FALSE, warning=FALSE}
data.frame(Model = c("Single predictor", "Linear Regression", "Knn", 
                     "Regression trees", "Random forest",
                     "LDA", "QDA"),
          Accuracy = c(percent(mean(p_hat_ens  == test_set$type), accuracy = 0.1),
                        percent(mean(p_hat_lm   == test_set$type), accuracy = 0.1),
                        percent(mean(p_knn      == test_set$type), accuracy = 0.1),
                        percent(mean(p_rpart    == test_set$type), accuracy = 0.1),
                        percent(mean(p_rf       == test_set$type), accuracy = 0.1),
                        percent(mean(p_lda[[1]] == test_set$type), accuracy = 0.1),
                        percent(mean(p_qda[[1]] == test_set$type), accuracy = 0.1)),
           
        Sensitivity = c(percent(sensitivity(p_hat_ens,  test_set$type), accuracy = 0.1),
                       percent(sensitivity(p_hat_lm,   test_set$type), accuracy = 0.1),
                       percent(sensitivity(p_knn,      test_set$type), accuracy = 0.1), 
                       percent(sensitivity(p_rpart,    test_set$type), accuracy = 0.1), 
                       percent(sensitivity(p_rf,       test_set$type), accuracy = 0.1), 
                       percent(sensitivity(p_lda[[1]], test_set$type), accuracy = 0.1), 
                       percent(sensitivity(p_qda[[1]], test_set$type), accuracy = 0.1)), 
           
       Specificity = c(percent(specificity(p_hat_ens,  test_set$type), accuracy = 0.1),
                       percent(specificity(p_hat_lm,   test_set$type), accuracy = 0.1),
                       percent(specificity(p_knn,      test_set$type), accuracy = 0.1),
                       percent(specificity(p_rpart,    test_set$type), accuracy = 0.1),
                       percent(specificity(p_rf,       test_set$type), accuracy = 0.1),
                       percent(specificity(p_lda[[1]], test_set$type), accuracy = 0.1),
                       percent(specificity(p_qda[[1]], test_set$type), accuracy = 0.1)))
```

##3.17. Cross Validation and Ensemble:##

All models used previously ran on a single partition of the training set, thereby unable to perfectly identify the different wine type therefore the application of cross validation on 11 classification algorithms and then combining the results would be deployed.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Now we are going to do several things:
# 1. train 10 classification models,
# 2. make the predictions for each model
# 3. calculate some statistics and store in the 'results' table
# 4. plot the ROC and precision-recall curves
# Then, we're going to plot the values in the 'results' table
# and make the ensemble of all models together.

# Load the packages used in this section
# Package "pROC" creates ROC and precision-recall plots
load_lib(c("pROC", "plotROC"))

# Several machine learning libraries
load_lib(c("e1071", "dplyr", "fastAdaboost", "gam", 
           "gbm", "import", "kernlab", "kknn", "klaR", 
           "MASS", "mboost", "mgcv", "monmlp", "naivebayes", "nnet", "plyr", 
           "ranger", "randomForest", "Rborist", "RSNNS", "wsrf"))


# Define models
models <- c("glm", "lda", "naive_bayes", "svmLinear", "rpart",
            "knn", "gamLoess", "multinom", "qda", "rf", "adaboost")

# We run cross validation in 10 folds, training with 90% of the data.
# We save the prediction to calculate the ROC and precision-recall curves
# and we use twoClassSummary to compute the sensitivity, specificity and 
# area under the ROC curve
contrl <- trainControl(method = "cv", number = 10, p = .9,
                        summaryFunction = twoClassSummary, 
                        classProbs = TRUE,
                        savePredictions = TRUE)

contrl <- trainControl(method = "cv", number = 10, p = .9,
                        classProbs = TRUE,
                        savePredictions = TRUE)

# Create 'results' table. The first row
# contains NAs and will be removed after
# the training
results <- tibble(Model = NA,
                  Accuracy = NA,
                  Sensitivity = NA,
                  Specificity = NA,
                  F1_Score = NA,
                  AUC = NA)

```

#3.17.1. Train the models:#

At this point the models would be trained, predictions made, stats calculated and stored in â€˜resultsâ€™ table. Hence the use of standard tuning parameters for all models except knn and random forest.

```{r echo=TRUE, message=FALSE, warning=FALSE}
#-------------------------------
# Start parallel processing
#-------------------------------
# The 'train' function in the 'caret' package allows the use of
# parallel processing. Here we enable this before training the models.
# See this link for details:
# http://topepo.github.io/caret/parallel-processing.html
cores <- 4    # Number of CPU cores to use
# Load 'doParallel' package for parallel processing
load_lib("doParallel")
cl <- makePSOCKcluster(cores)
registerDoParallel(cl)
set.seed(1234, sample.kind = "Rounding")
# Formula used in predictions
pml <- as.formula(paste("type", "~", 
                         paste(xcol, collapse=' + ')))

# Run predictions
preds <- sapply(models, function(model){ 
  
  if (model == "knn") {
    # knn use custom tuning parameters
    grid <- data.frame(k = seq(3, 50, 2))
    ft <- caret::train(form = pml, 
                        method = model, 
                        data = train_set, 
                        trControl = contrl,
                        tuneGrid = grid)
  } else if (model == "rforest") {
    # Random forest use custom tuning parameters
    t_grid <- data.frame(mtry = c(1, 2, 3, 4, 5, 10, 25, 50, 100))
    
    ft <- caret::train(form = pml,
                        method = "rforest", 
                        data = train_set,
                        trControl = contrl,
                        ntree = 150,
                        tuneGrid = t_grid,
                        nSamp = 5000)
  } else {
    # Other models use standard parameters (no tuning)
    ft <- caret::train(form = pml, 
                        method = model, 
                        data = train_set, 
                        trControl = contrl)
  }
  
  # Predictions
  pred <- predict(object = ft, newdata = test_set)
  
  # Accuracy
  acc <- mean(pred == test_set$type)
  
  # Sensitivity
  sen <- sensitivity(data = pred, 
                     reference = test_set$type, 
                     positive = "red")
  # Specificity
  spe <- specificity(data = pred, 
                     reference = test_set$type, 
                     positive = "red")
  
  # F1 score
  f1 <- F_meas(data = factor(pred), reference = test_set$type)
  
  # AUC
  auc_val <- auc(ft$pred$obs, ft$pred$red)
  
  # Store stats in 'results' table
  results <<- rbind(results,
                    tibble(
                      Model = model,
                      Accuracy = acc,
                      Sensitivity = sen,
                      Specificity = spe,
                      AUC = auc_val,
                      F1_Score = f1))

  # The predictions will be used for ensemble
  return(pred)
}) 

# Remove the first row of 'results' that contains NAs
results <- results[2:(nrow(results)),]
```

The combination of the best results of all models in a single ensemble is presented and the summarry of the results table.

```{r}

#-------------------------------
# Combine all models
#-------------------------------
# Use votes method to ensemble the predictions
votes <- rowMeans(preds == "red")
p_hat <- factor(ifelse(votes > 0.5, "red", "white"))

# Update the 'results' table
results <<- rbind(results,
                  tibble(
                    Model = "Ensemble",
                    Accuracy = mean(p_hat == test_set$type),
                    Sensitivity = sensitivity(p_hat, test_set$type),
                    Specificity = specificity(p_hat, test_set$type),
                    AUC = auc(p_hat, as.numeric(test_set$type)),
                    F1_Score = F_meas(p_hat, test_set$type)))

as_hux(results,
    add_colnames = TRUE) %>%
  # Format header row
  set_bold(row = 1, everywhere, value = TRUE)          %>%
  set_top_border(row = 1, everywhere, value = 1)       %>%
  set_bottom_border(row = c(1,13), everywhere, value = 1)    %>%
  # Format numbers
  set_number_format(row = -1, col = 2:6, value = 3)  %>%
  # Format alignment
  set_align(row = everywhere, col = 1,   value = 'left')  %>%
  set_align(row = everywhere, col = 2:6, value = 'right') %>%
  # Title
  set_caption('Model Performance With Cross Validation') %>%
  set_position(value = "center")
```

Generalized linear model (glm) has the best overall performance, than the ensemble.

```{r}
hux(Accuracy  = results[which.max(results$Accuracy),1]$Model,
    Sensitivity = results[which.max(results$Sensitivity),1]$Model,
    Specificity = results[which.max(results$Specificity),1]$Model,
    F_1   = results[which.max(results$F1_Score),1]$Model,
    AUC  = results[which.max(results$AUC),1]$Model,
    add_colnames = TRUE) %>%
    # Format header row
    set_bold(row = 1, col = everywhere, value = TRUE)        %>%
    set_top_border(row = 1, col = everywhere, value = 1)     %>%
    set_bottom_border(row = c(1,2), col = everywhere, value = 1)  %>%
    # Format table
    set_width(value = 0.6)                                   %>%
    set_caption("Best model")                                %>%
    set_position(value = "center")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#-------------------------------
# Plot the 'results' table
#-------------------------------
# We create a grid with all plots together.
# Each plot is simple Model vs Stats
results %>% 
  # Convert columns to lines
  pivot_longer(cols = 2:6, names_to = "Metric", values_drop_na = TRUE) %>%
  ggplot(aes(x = Model, y = value, group = 1)) + 
      geom_line() +
      geom_point() +
      # Y axis scale
      ylim(0.75, 1) +
      # Format labels
      ggtitle("Model performance") + 
      ylab("") +
      theme(legend.position="none" ,
            axis.text.x = element_text(angle = 90)) +
      # Arrange in grid
    facet_wrap(~Metric)
```

```{r}
results
```

##3.10 Predicting Quality:##

There are 5 samples of quality level 9 in the red and white datasets combined and no sample in the train_set_r dataset while the ubiquity or prevalence of quality levels 3, 4 and 8 is very low in the dataset.

The levels were previously combined to form the low, medium and high quality categories in the train_set_r dataset. The use of cross validation with ensemble would be deployed to predict the combined levels.The models such as glm, gamLoess, qda and adaboost would not work in this case.

```{r}
set.seed(1234, sample.kind = "Rounding")
# Formula used in predictions
pml_qual <- as.formula(paste("quality2", "~", 
                             paste(xcol, collapse=' + ')))

# Define models
#"glm",gamLoess, qda, adaboost
models <- c( "lda", "naive_bayes", "svmLinear", "rpart",
            "knn", "multinom", "rf")

# Create 'results' table. The first row
# contains NAs and will be removed after
# the training
q_results <- tibble(Model = NA,
                          Quality = NA,
                          Accuracy = NA,
                          Sensitivity = NA,
                          Specificity = NA,
                          F1_Score = NA)

preds_q <- sapply(models, function(model){ 
  
  print(model)
  if (model == "knn") {
    # knn use custom tuning parameters
    grid <- data.frame(k = seq(3, 50, 2))
    ft <- caret::train(form = pml_qual, 
                        method = model, 
                        data = train_set_r, 
                        trControl = contrl,
                        tuneGrid = grid)
  } else if (model == "rforest") {
    # Random forest use custom tuning parameters
    grid <- data.frame(mtry = c(1, 2, 3, 4, 5, 10, 25, 50, 100))
    
    ft <- caret::train(form = pml_qual,
                        method = "rforest", 
                        data = train_set_r,
                        trControl = contrl,
                        ntree = 150,
                        tuneGrid = grid,
                        nSamp = 5000)
  } else {
    # Other models use standard parameters (no tuning)
    ft <- caret::train(form = pml_qual, 
                        method = model, 
                        data = train_set_r, 
                        trControl = contrl)
  }
  
  # Predictions
  pred <- predict(object = ft, newdata = test_set_r)
  
  # Accuracy
  acc <- mean(pred == test_set_r$quality2)

  # Sensitivity
  sen <- caret::confusionMatrix(pred,
                                test_set_r$quality2)$byClass[,"Sensitivity"]
  # Specificity
  spe <- caret::confusionMatrix(pred,
                                test_set_r$quality2)$byClass[,"Specificity"]
  
  # F1 score
  f1 <- caret::confusionMatrix(pred,
                               test_set_r$quality2)$byClass[,"F1"]
  
  # Store stats in 'results' table
  q_results <<- rbind(q_results,
                            tibble(Model = model, 
                                   Quality = levels(test_set_r$quality2),
                                   Accuracy = acc,
                                   Sensitivity = sen,
                                   Specificity = spe,
                                   F1_Score = f1))

  # The predictions will be used for ensemble
  return(pred)
}) 

# Remove the first row of 'results' that contains NAs
q_results <- q_results[2:(nrow(q_results)),]

```

The combination of the best results of all models in a single ensemble is presented and the summarized table of results.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#-------------------------------
# Combine all models
#-------------------------------
# Use votes method to ensemble the predictions
votes <- data.frame(low    = rowSums(preds_q =="low"),
                    medium = rowSums(preds_q =="medium"),
                    high   = rowSums(preds_q =="high"))

p_hat <- factor(sapply(1:nrow(votes), function(x)
  colnames(votes[which.max(votes[x,])])))

p_hat <- relevel(p_hat, "medium")

  # Accuracy
  acc <- caret::confusionMatrix(p_hat,
                                test_set_r$quality2)$overall["Accuracy"]

  # Sensitivity
  sen <- caret::confusionMatrix(p_hat,
                                test_set_r$quality2)$byClass[,"Sensitivity"]
  # Specificity
  spe <- caret::confusionMatrix(p_hat,
                                test_set_r$quality2)$byClass[,"Specificity"]
  
  # F1 score
  f1 <- caret::confusionMatrix(p_hat,
                               test_set_r$quality2)$byClass[,"F1"]

  
q_results <<- rbind(q_results,
                          tibble(Model = "Ensemble",
                                 Quality = levels(test_set_r$quality2),
                                 Accuracy = acc,
                                 Sensitivity = sen,
                                 Specificity = spe,
                                 F1_Score = f1))

```
```{r echo=FALSE, message=FALSE, warning=FALSE}
#-------------------------------
# Plot the 'results' table
#-------------------------------
# Make a plot for each metric with the 3 quality levels
# The plots are grouped using the chunk options.

# Metric names
metrics <- names(q_results)[3:6]

res <- sapply(metrics, function(var){
  
  # Plot stored in 'p'
  z <- q_results %>% 
    # Convert columns to lines
    pivot_longer(cols = 3:6, names_to = "Metric", 
                 values_drop_na = TRUE) %>%
    
    pivot_wider(names_from = Quality) %>%
    filter(Metric == var) %>%
    
    ggplot(aes(x = Model, group = 1)) + 
        # Draw lines
        geom_line(aes(y = low,     col = "low")) +
        geom_line(aes(y = medium,  col = "medium")) +
        geom_line(aes(y = high,    col = "high")) +
        # Draw points
        geom_point(aes(y = low,    col = "low")) +
        geom_point(aes(y = medium, col = "medium")) +
        geom_point(aes(y = high,   col = "high")) +
        # Format labels
        ggtitle(var) + 
        ylab("Model") +
        theme(axis.text.x = element_text(angle = 90))

    # Show the plot
    print(z)
})

```

```{r}
#-------------------------------
# Stop parallel processing used in 'train'
#-------------------------------
stopCluster(cl)
```
##3.11 Clustering:## 

The unsupervised learning are used to identify possible clusters. Partitioning methods, such as k-means clustering require the users to specify the number of clusters to be generated. The function fviz_nbclust from the factoextra package determines and visualize the optimal number of clusters using different methods. For the purpose of this analysis clusters are created for the red wines because the properties of the white and the red wines are different. 


```{r echo=FALSE, message=FALSE, warning=FALSE}
#-------------------------------
# k-means
#-------------------------------
# Reference:
# https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/

# Load 'factoextra' to visualize clusters
load_lib("factoextra")

# Determine and visualize the optimal number of clusters 
# using total within sum of square (method = "wss")
train_set %>% filter(type == "red") %>% .[,xcol] %>%
  fviz_nbclust(x = ., FUNcluster = kmeans, method = "wss") + 
  geom_vline(xintercept = 4, linetype =2) +
  scale_y_continuous(labels = comma)
```

From the result of the plot it is observed that the total sum of squares decreases slowly for more than 4 clusters. The Selection of a larger value provides little improvement and increases the overlaps between the clusters.

```{r}
# We use 25 random starts for the clusters
k <- train_set %>% filter(type == "red") %>% .[,xcol] %>%
      kmeans(x = ., centers = 4, nstart = 25)

# Calculate cluster means
clus_m <- as_hux(data.frame(t(k$centers)), add_rownames = TRUE)
colnames(clus_m) <- c("Feature", paste("Cluster", 1:4))
clus_m <- add_colnames(clus_m)  
clus_m %>%
      # Format header row
      set_bold(row = 1, col = everywhere, value = TRUE)        %>%
      set_top_border(row = 1, col = everywhere, value = 1)     %>%
      set_bottom_border(row = c(1,12), col = everywhere, value = 1)  %>%
      # Format cells
      set_align(row = everywhere, col = 1,   value = 'left')   %>%
      set_align(row = everywhere, col = 2:5, value = 'right')  %>%
      set_number_format(row = 2:nrow(clus_m), 
                        col = everywhere, value = 3)           %>% 
      # Format table
      set_width(value = 0.7)                                   %>%
      set_position(value = "center")                           %>%
      set_caption("Features of Cluster Center")
```
The fviz_cluster function plots the clusters for the combination of two features. The clusters for total sulfur dioxide and chlorides are hereby ploted

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Plot the cluster
train_set %>% filter(type == "red") %>% .[,xcol] %>%
  fviz_cluster(object = k, 
               choose.vars = c("chlorides", "total_sulfur_dioxide"),
               geom   = "point", 
               repel  = TRUE, 
               main   = "Cluster plot with selected features",
               xlab   = "Chlorides",
               ylab   = "Total Sulfur Dioxide")
```

###IV. Conclusion:###


This research has explored the physicochemical constituent of red and white wine base on quality and type. According to experts quality is a subjective measure. The report gives a brief summary of model evaluation, explaining the most common metrics used in categorical problems in machine learning. The steps taken in actualizing the research includes, data preparation by downloading and importing the dataset, training and testing the dataset, data exploration and visualization of the features, model building ranging fron simple to a more complex models such as simple linear regression, LDA and QDA, CART using rpart, esemble and cross validation. The result obtained showed that the best predictors have low distribution overlapping area and low correlation among them. The model performance were evaluated and the algorithms were used to predict wine type and quality. Finaly, the utilization of the data to demonstrate clustering.
   
###V. Recommendation:###


The application of a more efficient machine learning models that would be able to predict red and white wine quality with high accuracy, specificity and sensitivity. 

More information on composition of grape varieties in each wine, the mix of experts that evaluated the wine quality and the production year would enrich the dataset and expand its scope.
A further research to ascertain the relationships between the different physicochemical properties of the wine and its association and correlation would clarify the issue of Simpson paradox. 


###References###
Rafael A. Irizarry, (2020) - â€œIntroduction to Data Scienceâ€ - https://rafalab.github.io/dsbook/

Baptiste AuguiÃ©, (2019) - â€œLaying out multiple plots on a pageâ€ - https://cran.r-project.org/web/packages/egg/vignettes/Ecosystem.html

Yihui Xie, J. J. Allaire, Garrett Grolemund (2019) - â€œR Markdown: The Definitive Guideâ€ - https://bookdown.org/yihui/rmarkdown/

Baptiste Auguie, (2017) - â€œArranging multiple grobs on a pageâ€ https://cran.r-project.org/web/packages/gridExtra/vignettes/arrangeGrob.html

Max Kuhn, (2019) - â€œThe caret Packageâ€ https://topepo.github.io/caret/model-training-and-tuning.html

ROC and AUC - https://stackoverflow.com/questions/31138751/roc-curve-from-training-data-in-caret

Xavier Robin et al, (2019), â€œDisplay and Analyze ROC Curvesâ€ - https://cran.r-project.org/web/packages/pROC/pROC.pdf

Sarang Narkhede. (2018) - â€œUnderstanding AUC - ROC Curveâ€ https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5

Alboukadel Kassambara, Fabian Mundt, (2019) - â€œExtract and Visualize the Results of Multivariate Data Analysesâ€ - https://cran.r-project.org/web/packages/factoextra/factoextra.pdf

â€œK-Means Clustering in R: Algorithm and Practical Examplesâ€ https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/

Teru Watanabe, (2016), â€œQuality and Physicochemical Properties of White Wineâ€ - https://rstudio-pubs-static.s3.amazonaws.com/152060_f4b5dad9dbd742a383ac3b8cee4e679c.html

Hadley Wickham, â€œThe Split-Apply-Combine Strategy for Data Analysisâ€ https://vita.had.co.nz/papers/plyr.pdf

https://stackoverflow.com/questions/57381627/add-text-labels-at-the-top-of-density-plots-to-label-the-different-groups

â€œWhat is Wine?â€ https://waterhouse.ucdavis.edu/tags/what-wine?page=1

â€œCluster Analysis in Râ€ http://girke.bioinformatics.ucr.edu/GEN242/pages/mydoc/Rclustering.html#1_introduction

https://www.edx.org/professional-certificate/harvardx-data-scienceâ†©

https://en.wikipedia.org/wiki/Vinho_Verdeâ†©

https://archive.ics.uci.edu/ml/datasets/Wine+Quality

http://gim.unmc.edu/dxtests/roc3.html

https://archive.ics.uci.edu/ml/datasets/Wine+Quality

https://portal.vinhoverde.pt/pt/estatisticas

https://topepo.github.io/caret/variable-importance.html

https://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/â†©

https://scikit-learn.org/stable/modules/lda_qda.htmlâ†©

/pagebreak
